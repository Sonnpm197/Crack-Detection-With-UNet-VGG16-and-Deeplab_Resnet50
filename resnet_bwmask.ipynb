{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b04b7d9-e6c3-4b79-8319-b047770d4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 448\n",
    "W = 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543de164-c317-41aa-8e87-7ec0051e3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# import shutil\n",
    "\n",
    "# # Paths to the folders containing images and masks\n",
    "# image_folder = 'kraggle_images/images'  # Folder with images\n",
    "# mask_folder = 'kraggle_images/masks'    # Folder with masks\n",
    "\n",
    "# # Destination folders for training and testing splits\n",
    "# train_image_folder = 'kraggle_images/training/images'\n",
    "# train_mask_folder = 'kraggle_images/training/masks'\n",
    "# test_image_folder = 'kraggle_images/testing/images'\n",
    "# test_mask_folder = 'kraggle_images/testing/masks'\n",
    "\n",
    "# # Create destination folders if they don't exist\n",
    "# os.makedirs(train_image_folder, exist_ok=True)\n",
    "# os.makedirs(train_mask_folder, exist_ok=True)\n",
    "# os.makedirs(test_image_folder, exist_ok=True)\n",
    "# os.makedirs(test_mask_folder, exist_ok=True)\n",
    "\n",
    "# # Get list of all images\n",
    "# image_list = os.listdir(image_folder)\n",
    "# mask_list = os.listdir(mask_folder)\n",
    "\n",
    "# # Make sure images and masks are paired correctly\n",
    "# image_list = sorted(image_list)\n",
    "# mask_list = sorted(mask_list)\n",
    "\n",
    "# # Split ratio for training and testing\n",
    "# train_ratio = 0.8\n",
    "# num_images = len(image_list)\n",
    "# train_size = int(train_ratio * num_images)\n",
    "\n",
    "# # Shuffle and split the data\n",
    "# combined = list(zip(image_list, mask_list))\n",
    "# random.shuffle(combined)\n",
    "\n",
    "# train_data = combined[:train_size]\n",
    "# test_data = combined[train_size:]\n",
    "\n",
    "# # Helper function to copy files\n",
    "# def copy_files(data, src_image_folder, src_mask_folder, dst_image_folder, dst_mask_folder):\n",
    "#     for image_name, mask_name in data:\n",
    "#         # Source paths\n",
    "#         image_src = os.path.join(src_image_folder, image_name)\n",
    "#         mask_src = os.path.join(src_mask_folder, mask_name)\n",
    "\n",
    "#         # Destination paths\n",
    "#         image_dst = os.path.join(dst_image_folder, image_name)\n",
    "#         mask_dst = os.path.join(dst_mask_folder, mask_name)\n",
    "\n",
    "#         # Copy image and mask to the respective folder\n",
    "#         shutil.copy(image_src, image_dst)\n",
    "#         shutil.copy(mask_src, mask_dst)\n",
    "\n",
    "# # Copy training data\n",
    "# copy_files(train_data, image_folder, mask_folder, train_image_folder, train_mask_folder)\n",
    "\n",
    "# # Copy testing data\n",
    "# copy_files(test_data, image_folder, mask_folder, test_image_folder, test_mask_folder)\n",
    "\n",
    "# print(f'Training set size: {len(train_data)} images')\n",
    "# print(f'Testing set size: {len(test_data)} images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "251be651-d96b-47b6-b00c-6116a2dd118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "Label = namedtuple( 'Label' , ['name' ,'id','color'] )\n",
    "labels = [\n",
    "    Label(  'ground'  ,  0 ,  (0,  0,  0)     ),\n",
    "    Label(  'crack'   ,  1 ,  (255, 255, 255) )\n",
    "]\n",
    "\n",
    "max_label_id = max([l.id for l in labels])\n",
    "\n",
    "# turn to arr of numbers between 0 to 1\n",
    "LabelCmap = colors.ListedColormap([np.array(l.color) / 255 for l in labels])\n",
    "bounds = np.linspace(0, max_label_id, len(labels))\n",
    "\n",
    "def display(display_list):\n",
    "  fig, cols = plt.subplots(1, len(display_list), sharey=True)\n",
    "  fig.set_size_inches(15, 5)\n",
    "  title = ['Input Image', 'Ground-truth Labels', 'Stage 1 Prediction Labels']\n",
    "  im = None\n",
    "  for i in range(len(display_list)):\n",
    "    cols[i].set_title(title[i])\n",
    "    y = display_list[i]\n",
    "    \n",
    "    if 'Label' in title[i]:\n",
    "      w,h,_ = y.shape\n",
    "      y = y.reshape(w,h)\n",
    "      im = cols[i].imshow(y, cmap=LabelCmap, vmin=0, vmax=max_label_id)\n",
    "    else:\n",
    "      im = cols[i].imshow(tf.keras.preprocessing.image.array_to_img(y)) # this look better\n",
    "      #im = cols[i].imshow(y)\n",
    "    cols[i].axis('off')\n",
    "  cbar = plt.colorbar(im, ax=cols.ravel().tolist(), cmap=LabelCmap, norm=colors.BoundaryNorm(bounds, LabelCmap.N), \n",
    "                      orientation='horizontal',spacing='proportional', ticks=bounds, boundaries=bounds, format='%1i')\n",
    "  cbar.ax.set_xticklabels([l.name for l in labels], rotation=90)\n",
    "  plt.show()\n",
    "\n",
    "# Plotting Loss, IoU, and Correct in separate subplots\n",
    "def plot_metrics(mode, loss_history, iou_history, correct_history):\n",
    "    # print(loss_history)\n",
    "    # print(iou_history)\n",
    "    # print(correct_history)\n",
    "    epochs = range(1, len(loss_history) + 1)\n",
    "\n",
    "    # Create subplots: 1 row, 3 columns\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Plot Loss\n",
    "    ax1.plot(epochs, np.array(loss_history), color='tab:blue', label='Loss')\n",
    "    ax1.set_title(f\"{mode} Loss Over Epochs\")\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot IoU\n",
    "    #print(iou_history)\n",
    "    ax2.plot(epochs, [iou.cpu().numpy() for iou in iou_history], color='tab:green', label='IoU')\n",
    "    ax2.set_title(f\"{mode} IoU Over Epochs\")\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('IoU')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Plot Correct Predictions\n",
    "    ax3.plot(epochs, np.array(correct_history), color='tab:red', label='Correct Predictions')\n",
    "    ax3.set_title(f\"{mode} Correct Predictions Over Epochs\")\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('Correct Predictions')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax3.legend()\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plots\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78743400-6296-45a8-9da3-50cbf8486543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import scipy.io as scio\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "class CrackDataSet(torch.utils.data.Dataset):\n",
    "    # data[0] is image, data[1] is mask\n",
    "    def __init__(self, data, data_transforms):\n",
    "        self.data_transforms = data_transforms\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data[0]) # images size\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # OpenCV's default = BGR => RGB and then adding normalization here\n",
    "        image = cv2.imread(self.data[0][index], cv2.IMREAD_COLOR)\n",
    "        image = self.data_transforms(Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('uint8'), 'RGB'))\n",
    "\n",
    "        # masks are black and white images, so pixels closer to 255 consider = crack = label them\n",
    "        mask = cv2.imread(self.data[1][index], cv2.IMREAD_GRAYSCALE)\n",
    "        mask_label = np.zeros(mask.shape)\n",
    "        mask_label[mask >= 128] = 1\n",
    "        mask_label = np.asarray(mask_label, np.compat.long)\n",
    "        mask_label = torch.LongTensor(mask_label)\n",
    "        return image, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0ce063-df35-4694-8b5b-846e62098863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import JaccardIndex\n",
    "import shutil\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def load_data(path=''):\n",
    "    images = sorted(glob(os.path.join(path, \"images/*\")))\n",
    "    masks = sorted(glob(os.path.join(path, \"masks/*\")))\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "    return (train_x, train_y), (valid_x, valid_y)\n",
    "\n",
    "class Trainer_Wrapper():\n",
    "    def __init__(self, model, input_path, batch_size, max_epochs, num_class=5):\n",
    "        self.num_class = num_class\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = torch.device(\"cpu\")\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = 1  # An epoch in machine learning is one complete pass through the entire training dataset during the training process\n",
    "        self.max_epochs = max_epochs\n",
    "        self.lr = 0.001  # Learning rate for the optimizer\n",
    "        data_train, data_eval = load_data(path=input_path)\n",
    "        self.image_datasets = {\n",
    "            'train':\n",
    "                CrackDataSet(data_train, data_transforms['train']),\n",
    "            'validation':\n",
    "                CrackDataSet(data_eval, data_transforms['validation'])\n",
    "        }\n",
    "        # create dataloader to read batch_size of items for training\n",
    "        # when training, images should be choose random, so shuffle = True\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.image_datasets['train'],\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True)\n",
    "        self.val_loader = torch.utils.data.DataLoader(self.image_datasets['validation'],\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shuffle=False)\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)  # gradient descent algorithm\n",
    "        #self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=10, threshold=0.0001, threshold_mode='abs')\n",
    "\n",
    "        # Intersection over Union (IoU): Measures the overlap between the predicted segmentation and the ground truth.\n",
    "        self.fiou = JaccardIndex(task='multiclass', num_classes=self.num_class).to(self.device)\n",
    "        self.loss_criterion = nn.CrossEntropyLoss()\n",
    "        self.train_loss_history = []\n",
    "        self.train_iou_history = []\n",
    "        self.train_correct_history = []\n",
    "        self.eval_loss_history = []\n",
    "        self.eval_iou_history = []\n",
    "        self.eval_correct_history = []\n",
    "\n",
    "    def begin_train(self):\n",
    "        start = self.epoch\n",
    "        for self.epoch in range(start, self.max_epochs + 1):\n",
    "            self.training()\n",
    "            self.evaluating()\n",
    "\n",
    "        plot_metrics(\"Training\", self.train_loss_history, self.train_iou_history, self.train_correct_history)\n",
    "        plot_metrics(\"Evaluating\", self.eval_loss_history, self.eval_iou_history, self.eval_correct_history)\n",
    "\n",
    "    def training(self):\n",
    "        self.model.train()\n",
    "        total_correct = 0\n",
    "        # Wraps the DataLoader with tqdm for a progress bar during training.\n",
    "        total_iou, total_loss, total_num_of_images, data_bar = 0.0, 0.0, 0.0, tqdm(self.train_loader)\n",
    "        torch.enable_grad()  # Ensures that gradients are computed.\n",
    "        num_of_batches = 0\n",
    "        loss_arr, iou_arr, correct_arr  = [], [], []\n",
    "        for data, target in data_bar:\n",
    "            num_of_batches += 1\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "            # ensures that all CUDA operations on the GPU have completed\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            out = self.model(data)\n",
    "            out = out['out']\n",
    "\n",
    "            # Converts the model's output logits to predicted class labels.\n",
    "            # for example, tensor([[0.1, 2.5, 0.3],  # Sample 1 (class 1 has the highest score)\n",
    "            #                     [1.2, 0.4, 3.0]]) # Sample 2 (class 2 has the highest score)\n",
    "            # then calling argmax will get tensor([1, 2])\n",
    "            prediction = torch.argmax(out, dim=1)\n",
    "\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            # loss between the predictions and ground truth.\n",
    "            loss = self.loss_criterion(out, target)\n",
    "\n",
    "            num_of_samples = data.size(0)\n",
    "            # quantifies the difference between the predicted output and the true values\n",
    "            total_loss += loss.item() * num_of_samples\n",
    "            # Calculates the Intersection over Union (IoU) for the predictions.\n",
    "            total_iou += self.fiou(out.data.to(self.device), target.data.to(self.device))\n",
    "            total_num_of_images += num_of_samples\n",
    "\n",
    "            # the model parameters approach a local minimum in the loss landscape.\n",
    "            # Reducing the learning rate helps the model make smaller, more precise updates, which can improve convergence to a minimum.\n",
    "            self.lr = self.lr * 0.9997\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = self.lr\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # For example, if target has the shape [32, 256, 256] (for a batch of 32 images with 256x256 pixel labels, as in a segmentation task):\n",
    "            # target.numel() would return 32 * 256 * 256 = 2,097,152, which is the total number of elements in the tensor.\n",
    "            total_correct += torch.sum(prediction == target).item() / target.numel() * num_of_samples\n",
    "            # mPA = mean Pixel Accuracy\n",
    "            \n",
    "            avg_loss = total_loss / total_num_of_images\n",
    "            avg_iou = total_iou / num_of_batches\n",
    "            avg_correct = total_correct / total_num_of_images\n",
    "\n",
    "            loss_arr.append(avg_loss)\n",
    "            iou_arr.append(avg_iou)\n",
    "            correct_arr.append(avg_correct)\n",
    "            \n",
    "            data_bar.set_description('Train epoch: [{}/{}] Loss: {:.4f} mPA: {:.2f}% IOU: {:.4f} '\n",
    "                                     .format(self.epoch, self.max_epochs, avg_loss, avg_correct, avg_iou))\n",
    "\n",
    "        self.train_loss_history.append(sum(loss_arr) / num_of_batches)\n",
    "        self.train_iou_history.append(sum(iou_arr) / num_of_batches)\n",
    "        self.train_correct_history.append(sum(correct_arr) / num_of_batches)\n",
    "\n",
    "    def evaluating(self):\n",
    "        self.model.eval() # enable eval mode\n",
    "        torch.no_grad()\n",
    "        total_iou, total_loss, total_correct, total_num_of_images, data_bar = 0.0, 0.0, 0.0, 0, tqdm(self.val_loader)\n",
    "        num_of_batches = 0\n",
    "\n",
    "        loss_arr, iou_arr, correct_arr  = [], [], []\n",
    "        \n",
    "        for data, target in data_bar:\n",
    "            num_of_batches += 1\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "            out = self.model(data)\n",
    "            out = out['out']\n",
    "\n",
    "            prediction = torch.argmax(out, dim=1)\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "            loss = self.loss_criterion(out, target)\n",
    "\n",
    "            num_of_samples = data.size(0)\n",
    "            total_loss += loss.item() * num_of_samples\n",
    "            total_iou += self.fiou(out.data.to(self.device), target.data.to(self.device))\n",
    "            total_num_of_images += num_of_samples\n",
    "            total_correct += torch.sum(prediction == target).item() / target.numel() * num_of_samples\n",
    "\n",
    "            avg_loss = total_loss / total_num_of_images\n",
    "            avg_iou = total_iou / num_of_batches\n",
    "            avg_correct = total_correct / total_num_of_images\n",
    "\n",
    "            loss_arr.append(avg_loss)\n",
    "            iou_arr.append(avg_iou)\n",
    "            correct_arr.append(avg_correct)\n",
    "            \n",
    "            data_bar.set_description('Eval epoch:  [{}/{}] Loss: {:.4f} mPA: {:.2f}% IOU: {:.4f} '\n",
    "                                     .format(self.epoch, self.max_epochs, avg_loss, avg_correct, avg_iou))\n",
    "            \n",
    "        self.eval_loss_history.append(sum(loss_arr) / num_of_batches)\n",
    "        self.eval_iou_history.append(sum(iou_arr) / num_of_batches)\n",
    "        self.eval_correct_history.append(sum(correct_arr) / num_of_batches)\n",
    "        #self.scheduler.step(sum(loss_arr) / num_of_batches)\n",
    "\n",
    "        #self.random_val_predict()\n",
    "\n",
    "    def random_val_predict(self, info=True, idx=0):\n",
    "        if (idx == 0):\n",
    "            idx = random.randint(0, len(self.image_datasets['validation']) - 1)\n",
    "\n",
    "        rgb, mask = self.image_datasets['validation'][idx]\n",
    "        # Suppose image is a tensor with shape [1, 3, 480, 640]:\n",
    "        # 1 is the batch dimension (batch size of 1).\n",
    "        # 3 is the number of color channels (RGB).\n",
    "        # 480 and 640 are the height and width of the image.\n",
    "        # squeeze(0): Removes the first dimension if its size is 1\n",
    "        image = rgb.unsqueeze(0).to(self.device)\n",
    "        mask = mask.unsqueeze(0).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():  # Disables gradient computation\n",
    "            label = mask.cpu().numpy()  # convert a PyTorch tensor to a NumPy array and ensure it is on the CPU\n",
    "            label = label.transpose(1, 2, 0)\n",
    "            label = label.reshape(H, W, 1)\n",
    "\n",
    "            img = image.squeeze(0)\n",
    "            img = img.cpu().numpy()\n",
    "            img = img.transpose(1, 2, 0)\n",
    "            if img.shape[2] > 3:\n",
    "                img = img[:, :, 0:3]\n",
    "\n",
    "            outputs = self.model(image)\n",
    "            outputs = outputs['out']\n",
    "            pred = torch.argmax(outputs, 1)  # Gets the predicted class labels.\n",
    "            pred = pred.squeeze(0).cpu().data.numpy()\n",
    "\n",
    "            display([img[:, :, 0:3], label, pred.reshape(H, W, 1)])\n",
    "        return img, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d82689d8-3b35-49ef-8031-3ec92bfdb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RUN THIS BEFORE AND AFTER YOU TRAIN AND EVAL YOUR MODEL TO FREE UP MEM, OTHERWISE YOU'LL END UP OOM ERROR\"\"\"\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521cd43-0481-4fef-b010-25a7da34a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bigda\\anaconda3\\envs\\py312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bigda\\anaconda3\\envs\\py312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Train epoch: [1/2] Loss: 0.0855 mPA: 0.97% IOU: 0.6404 :  51%|██████████▋          | 1529/3013 [13:40<13:09,  1.88it/s]"
     ]
    }
   ],
   "source": [
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from torchvision.models.segmentation import deeplabv3_mobilenet_v3_large\n",
    "import torchvision.models as models\n",
    "\n",
    "# input_path = 'kraggle_images/bw_unet_test'\n",
    "input_path = 'kraggle_images'\n",
    "batch_size = 3 # Number of samples per batch during training\n",
    "max_epochs = 2\n",
    "num_of_classes = 2\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([transforms.ToTensor(), normalize]),\n",
    "    'validation': transforms.Compose([transforms.ToTensor(), normalize]),\n",
    "}\n",
    "\n",
    "def custom_DeepLabv3():\n",
    "  model = deeplabv3_resnet50(pretrained=True, progress=True)\n",
    "  model.classifier = DeepLabHead(2048, 2)\n",
    "  return model\n",
    "\n",
    "model = custom_DeepLabv3()\n",
    "trainer = Trainer_Wrapper(model, input_path, batch_size, max_epochs, num_of_classes)\n",
    "trainer.begin_train()\n",
    "\n",
    "weight_dir = input_path + \"/weights\"\n",
    "filename = 'res_net_best_weight.pth'\n",
    "save_path = os.path.join(weight_dir, filename)\n",
    "torch.save(trainer.model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5962c-1913-45d1-8ada-241a1de53da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
