{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b04b7d9-e6c3-4b79-8319-b047770d4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 448\n",
    "W = 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d97e562-8a70-4c9c-9842-bb675e6904ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            # kernel_size=3: 3x3 filter to process the input.\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            # mid channels for more complex patterns, moving from simple edges (early layers) to higher-level features (middle layers) before reaching the final output\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "\n",
    "            # the filter (or kernel) in a convolutional layer is typically followed by an activation function like ReLU\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    # A 2x2 max pooling operation with a stride of 2 will reduce each 2x2 block of the input feature map to a single value.\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            # 2: This is the size of the pooling window 2x2, moves across the input and takes the maximum value from each 2x2 block.\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            # It enlarges the input feature map or image. Imagine stretching a picture to make it bigger.\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            # increases the size of the input feature maps. But it does so with learned parameters, \n",
    "            # meaning it can adapt and learn how to best increase the size during training.\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    # x2 > x1 in spatial dimensions (h + w), because x1 is being padded to match the size of x2 before concatenation\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down_64_to_128 = Down(64, 128)\n",
    "        self.down_128_to_256 = Down(128, 256)\n",
    "        self.down_256_to_512 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down_512_to_1024 = Down(512, 1024 // factor)\n",
    "        self.up_1024_to_512 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up_512_to_256 = Up(512, 256 // factor, bilinear)\n",
    "        self.up_256_to_128 = Up(256, 128 // factor, bilinear)\n",
    "        self.up_128_to_64 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down_64_to_128(x1)\n",
    "        x3 = self.down_128_to_256(x2)\n",
    "        x4 = self.down_256_to_512(x3)\n",
    "        x5 = self.down_512_to_1024(x4)\n",
    "        x = self.up_1024_to_512(x5, x4)\n",
    "        x = self.up_512_to_256(x, x3)\n",
    "        x = self.up_256_to_128(x, x2)\n",
    "        x = self.up_128_to_64(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "251be651-d96b-47b6-b00c-6116a2dd118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Label = namedtuple( 'Label' , ['name' ,'id','color'] )\n",
    "labels = [\n",
    "    Label(  'ground'  ,  0 ,  (0,  0,  0)     ),\n",
    "    Label(  'crack'   ,  1 ,  (255, 255, 255) )\n",
    "]\n",
    "\n",
    "max_label_id = max([l.id for l in labels])\n",
    "\n",
    "# turn to arr of numbers between 0 to 1\n",
    "LabelCmap = colors.ListedColormap([np.array(l.color) / 255 for l in labels])\n",
    "bounds = np.linspace(0, max_label_id, len(labels))\n",
    "\n",
    "def display(display_list):\n",
    "  fig, cols = plt.subplots(1, len(display_list), sharey=True)\n",
    "  fig.set_size_inches(15, 5)\n",
    "  title = ['Input Image', 'Ground-truth Labels', 'Stage 1 Prediction Labels']\n",
    "  im = None\n",
    "  for i in range(len(display_list)):\n",
    "    cols[i].set_title(title[i])\n",
    "    y = display_list[i]\n",
    "    \n",
    "    if 'Label' in title[i]:\n",
    "      w,h,_ = y.shape\n",
    "      y = y.reshape(w,h)\n",
    "      im = cols[i].imshow(y, cmap=LabelCmap, vmin=0, vmax=max_label_id)\n",
    "    else:\n",
    "      transform_to_pil = transforms.ToPILImage()\n",
    "      img = transform_to_pil(y)\n",
    "      im = cols[i].imshow(img)\n",
    "    cols[i].axis('off')\n",
    "  cbar = plt.colorbar(im, ax=cols.ravel().tolist(), cmap=LabelCmap, norm=colors.BoundaryNorm(bounds, LabelCmap.N), \n",
    "                      orientation='horizontal',spacing='proportional', ticks=bounds, boundaries=bounds, format='%1i')\n",
    "  cbar.ax.set_xticklabels([l.name for l in labels], rotation=90)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78743400-6296-45a8-9da3-50cbf8486543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import scipy.io as scio\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "class CrackDataSet(torch.utils.data.Dataset):\n",
    "    # data[0] is image, data[1] is mask\n",
    "    def __init__(self, data, data_transforms):\n",
    "        self.data_transforms = data_transforms\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data[0]) # images size\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # OpenCV's default = BGR => RGB and then adding normalization here\n",
    "        image = cv2.imread(self.data[0][index], cv2.IMREAD_COLOR)\n",
    "        image = self.data_transforms(Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('uint8'), 'RGB'))\n",
    "\n",
    "        # masks are black and white images, so pixels closer to 255 consider = crack = label them\n",
    "        mask = cv2.imread(self.data[1][index], cv2.IMREAD_GRAYSCALE)\n",
    "        mask_label = np.zeros(mask.shape)\n",
    "        mask_label[mask >= 128] = 1\n",
    "        mask_label = np.asarray(mask_label, np.compat.long)\n",
    "        mask_label = torch.LongTensor(mask_label)\n",
    "        return image, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e0ce063-df35-4694-8b5b-846e62098863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import JaccardIndex\n",
    "import shutil\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "def load_data(path=''):\n",
    "    images = sorted(glob(os.path.join(path, \"images/*\")))\n",
    "    masks = sorted(glob(os.path.join(path, \"masks/*\")))\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "    return (train_x, train_y), (valid_x, valid_y)\n",
    "\n",
    "class Trainer_Wrapper():\n",
    "    def __init__(self, model, input_path, batch_size, max_epochs, num_class=5):\n",
    "        self.num_class = num_class\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = 0  # An epoch in machine learning is one complete pass through the entire training dataset during the training process\n",
    "        self.max_epochs = max_epochs\n",
    "        self.lr = 0.001  # Learning rate for the optimizer\n",
    "        data_train, data_eval = load_data(path=input_path)\n",
    "        self.image_datasets = {\n",
    "            'train':\n",
    "                CrackDataSet(data_train, data_transforms['train']),\n",
    "            'validation':\n",
    "                CrackDataSet(data_eval, data_transforms['validation'])\n",
    "        }\n",
    "        # create dataloader to read batch_size of items for training\n",
    "        # when training, images should be choose random, so shuffle = True\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.image_datasets['train'],\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True)\n",
    "        self.val_loader = torch.utils.data.DataLoader(self.image_datasets['validation'],\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shuffle=False)\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)  # gradient descent algorithm\n",
    "\n",
    "        # Intersection over Union (IoU): Measures the overlap between the predicted segmentation and the ground truth.\n",
    "        self.fiou = JaccardIndex(task='multiclass', num_classes=self.num_class).to(self.device)\n",
    "        self.loss_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def begin_train(self):\n",
    "        start = self.epoch\n",
    "        for self.epoch in range(start, self.max_epochs + 1):\n",
    "            self.training()\n",
    "            self.evaluating()\n",
    "\n",
    "    def training(self):\n",
    "        self.model.train()\n",
    "        total_correct = 0\n",
    "        # Wraps the DataLoader with tqdm for a progress bar during training.\n",
    "        total_iou, total_loss, total_num_of_images, data_bar = 0.0, 0.0, 0.0, tqdm(self.train_loader)\n",
    "        torch.enable_grad()  # Ensures that gradients are computed.\n",
    "        num_of_batches = 0\n",
    "        for data, target in data_bar:\n",
    "            num_of_batches += 1\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "            # ensures that all CUDA operations on the GPU have completed\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            out = self.model(data)\n",
    "\n",
    "            # Converts the model's output logits to predicted class labels.\n",
    "            # for example, tensor([[0.1, 2.5, 0.3],  # Sample 1 (class 1 has the highest score)\n",
    "            #                     [1.2, 0.4, 3.0]]) # Sample 2 (class 2 has the highest score)\n",
    "            # then calling argmax will get tensor([1, 2])\n",
    "            prediction = torch.argmax(out, dim=1)\n",
    "\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            # loss between the predictions and ground truth.\n",
    "            loss = self.loss_criterion(out, target)\n",
    "\n",
    "            num_of_samples = data.size(0)\n",
    "            # quantifies the difference between the predicted output and the true values\n",
    "            total_loss += loss.item() * num_of_samples\n",
    "            # Calculates the Intersection over Union (IoU) for the predictions.\n",
    "            total_iou += self.fiou(out.data.to(self.device), target.data.to(self.device))\n",
    "            total_num_of_images += num_of_samples\n",
    "\n",
    "            # the model parameters approach a local minimum in the loss landscape.\n",
    "            # Reducing the learning rate helps the model make smaller, more precise updates, which can improve convergence to a minimum.\n",
    "            self.lr = self.lr * 0.9997\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = self.lr\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # For example, if target has the shape [32, 256, 256] (for a batch of 32 images with 256x256 pixel labels, as in a segmentation task):\n",
    "            # target.numel() would return 32 * 256 * 256 = 2,097,152, which is the total number of elements in the tensor.\n",
    "            total_correct += torch.sum(prediction == target).item() / target.numel() * num_of_samples\n",
    "            # mPA = mean Pixel Accuracy\n",
    "            data_bar.set_description('Train epoch: [{}/{}] Loss: {:.4f} mPA: {:.2f}% IOU1: {:.4f} '\n",
    "                .format(self.epoch, self.max_epochs, total_loss / total_num_of_images, total_correct / total_num_of_images, total_iou / num_of_batches))\n",
    "\n",
    "    def evaluating(self):\n",
    "        self.model.eval() # enable eval mode\n",
    "        torch.no_grad()\n",
    "        total_iou, total_loss, total_correct, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(self.val_loader)\n",
    "        num = 0\n",
    "        for data, target in data_bar:\n",
    "            num += 1\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "            out = self.model(data)\n",
    "\n",
    "            prediction = torch.argmax(out, dim=1)\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "            loss = self.loss_criterion(out, target)\n",
    "\n",
    "            num_of_samples = data.size(0)\n",
    "            total_loss += loss.item() * num_of_samples\n",
    "            total_iou += self.fiou(out.data.to(self.device), target.data.to(self.device))\n",
    "            total_num += num_of_samples\n",
    "            total_correct += torch.sum(prediction == target).item() / target.numel() * num_of_samples\n",
    "            data_bar.set_description('Eval epoch:  [{}/{}] Loss: {:.4f} mPA: {:.2f}% IOU1: {:.4f} '\n",
    "                                     .format(self.epoch, self.max_epochs, total_loss / total_num, total_correct / total_num, total_iou / num))\n",
    "\n",
    "        self.random_val_predict()\n",
    "\n",
    "    def random_val_predict(self, info=True, idx=0):\n",
    "        if (idx == 0):\n",
    "            idx = random.randint(0, len(self.image_datasets['validation']) - 1)\n",
    "\n",
    "        rgb, mask = self.image_datasets['validation'][idx]\n",
    "        # Suppose image is a tensor with shape [1, 3, 480, 640]:\n",
    "        # 1 is the batch dimension (batch size of 1).\n",
    "        # 3 is the number of color channels (RGB).\n",
    "        # 480 and 640 are the height and width of the image.\n",
    "        # squeeze(0): Removes the first dimension if its size is 1\n",
    "        image = rgb.unsqueeze(0).to(self.device)\n",
    "        mask = mask.unsqueeze(0).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():  # Disables gradient computation\n",
    "            label = mask.cpu().numpy()  # convert a PyTorch tensor to a NumPy array and ensure it is on the CPU\n",
    "            label = label.transpose(1, 2, 0)\n",
    "            label = label.reshape(H, W, 1)\n",
    "\n",
    "            img = image.squeeze(0)\n",
    "            img = img.cpu().numpy()\n",
    "            img = img.transpose(1, 2, 0)\n",
    "            if img.shape[2] > 3:\n",
    "                img = img[:, :, 0:3]\n",
    "\n",
    "            outputs = self.model(image)\n",
    "            pred = torch.argmax(outputs, 1)  # Gets the predicted class labels.\n",
    "            pred = pred.squeeze(0).cpu().data.numpy()\n",
    "\n",
    "            display([img[:, :, 0:3], label, pred.reshape(H, W, 1)])\n",
    "        return img, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82689d8-3b35-49ef-8031-3ec92bfdb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RUN THIS BEFORE AND AFTER YOU TRAIN AND EVAL YOUR MODEL TO FREE UP MEM, OTHERWISE YOU'LL END UP OOM ERROR\"\"\"\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4521cd43-0481-4fef-b010-25a7da34a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: [0/50] Loss: 0.6875 mPA: 0.63% IOU1: 0.3375 : 100%|█████████████████████████| 2/2 [00:10<00:00,  5.11s/it]\n",
      "Eval epoch:  [0/50] Loss: 0.6507 mPA: 0.98% IOU1: 0.4877 : 100%|█████████████████████████| 1/1 [00:00<00:00,  3.74it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'axes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet(\u001b[38;5;241m3\u001b[39m, num_of_classes, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer_Wrapper(model, input_path, batch_size, max_epochs, num_of_classes)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 55\u001b[0m, in \u001b[0;36mTrainer_Wrapper.begin_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining()\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluating\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 133\u001b[0m, in \u001b[0;36mTrainer_Wrapper.evaluating\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m     total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(prediction \u001b[38;5;241m==\u001b[39m target)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m target\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m*\u001b[39m num_of_samples\n\u001b[0;32m    130\u001b[0m     data_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEval epoch:  [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m mPA: \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m IOU1: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    131\u001b[0m                              \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs, total_loss \u001b[38;5;241m/\u001b[39m total_num, total_correct \u001b[38;5;241m/\u001b[39m total_num, total_iou \u001b[38;5;241m/\u001b[39m num))\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_val_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 163\u001b[0m, in \u001b[0;36mTrainer_Wrapper.random_val_predict\u001b[1;34m(self, info, idx)\u001b[0m\n\u001b[0;32m    160\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Gets the predicted class labels.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m--> 163\u001b[0m     \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, pred\n",
      "Cell \u001b[1;32mIn[10], line 35\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(display_list)\u001b[0m\n\u001b[0;32m     33\u001b[0m     transform_to_pil \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToPILImage()\n\u001b[0;32m     34\u001b[0m     img \u001b[38;5;241m=\u001b[39m transform_to_pil(y)\n\u001b[1;32m---> 35\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43maxes\u001b[49m[i]\u001b[38;5;241m.\u001b[39mimshow(img)\n\u001b[0;32m     36\u001b[0m   cols[i]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m cbar \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mcolorbar(im, ax\u001b[38;5;241m=\u001b[39mcols\u001b[38;5;241m.\u001b[39mravel()\u001b[38;5;241m.\u001b[39mtolist(), cmap\u001b[38;5;241m=\u001b[39mLabelCmap, norm\u001b[38;5;241m=\u001b[39mcolors\u001b[38;5;241m.\u001b[39mBoundaryNorm(bounds, LabelCmap\u001b[38;5;241m.\u001b[39mN), \n\u001b[0;32m     38\u001b[0m                     orientation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhorizontal\u001b[39m\u001b[38;5;124m'\u001b[39m,spacing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproportional\u001b[39m\u001b[38;5;124m'\u001b[39m, ticks\u001b[38;5;241m=\u001b[39mbounds, boundaries\u001b[38;5;241m=\u001b[39mbounds, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%1i\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'axes' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_path = 'kraggle_images/bw_unet_test'\n",
    "batch_size = 4 # Number of samples per batch during training\n",
    "max_epochs = 50\n",
    "num_of_classes = 2\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([transforms.ToTensor(), normalize]),\n",
    "    'validation': transforms.Compose([transforms.ToTensor(), normalize]),\n",
    "}\n",
    "\n",
    "#model = UNet(3, num_of_classes)\n",
    "model = UNet(3, num_of_classes, True)\n",
    "trainer = Trainer_Wrapper(model, input_path, batch_size, max_epochs, num_of_classes)\n",
    "trainer.begin_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5962c-1913-45d1-8ada-241a1de53da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
