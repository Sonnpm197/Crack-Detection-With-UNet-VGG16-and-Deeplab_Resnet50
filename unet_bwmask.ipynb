{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b04b7d9-e6c3-4b79-8319-b047770d4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 448\n",
    "W = 448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d97e562-8a70-4c9c-9842-bb675e6904ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            # kernel_size=3: 3x3 filter to process the input.\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            # mid channels for more complex patterns, moving from simple edges (early layers) to higher-level features (middle layers) before reaching the final output\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "\n",
    "            # the filter (or kernel) in a convolutional layer is typically followed by an activation function like ReLU\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    # A 2x2 max pooling operation with a stride of 2 will reduce each 2x2 block of the input feature map to a single value.\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            # 2: This is the size of the pooling window 2x2, moves across the input and takes the maximum value from each 2x2 block.\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            # It enlarges the input feature map or image. Imagine stretching a picture to make it bigger.\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            # increases the size of the input feature maps. But it does so with learned parameters, \n",
    "            # meaning it can adapt and learn how to best increase the size during training.\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    # x2 > x1 in spatial dimensions (h + w), because x1 is being padded to match the size of x2 before concatenation\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down_64_to_128 = Down(64, 128)\n",
    "        self.down_128_to_256 = Down(128, 256)\n",
    "        self.down_256_to_512 = Down(256, 512)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down_512_to_1024 = Down(512, 1024 // factor)\n",
    "        self.up_1024_to_512 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up_512_to_256 = Up(512, 256 // factor, bilinear)\n",
    "        self.up_256_to_128 = Up(256, 128 // factor, bilinear)\n",
    "        self.up_128_to_64 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down_64_to_128(x1)\n",
    "        x3 = self.down_128_to_256(x2)\n",
    "        x4 = self.down_256_to_512(x3)\n",
    "        x5 = self.down_512_to_1024(x4)\n",
    "        x = self.up_1024_to_512(x5, x4)\n",
    "        x = self.up_512_to_256(x, x3)\n",
    "        x = self.up_256_to_128(x, x2)\n",
    "        x = self.up_128_to_64(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "251be651-d96b-47b6-b00c-6116a2dd118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "Label = namedtuple( 'Label' , ['name' ,'id','color'] )\n",
    "labels = [\n",
    "    Label(  'ground'  ,  0 ,  (0,  0,  0)     ),\n",
    "    Label(  'crack'   ,  1 ,  (255, 255, 255) )\n",
    "]\n",
    "\n",
    "max_label_id = max([l.id for l in labels])\n",
    "\n",
    "# turn to arr of numbers between 0 to 1\n",
    "LabelCmap = colors.ListedColormap([np.array(l.color) / 255 for l in labels])\n",
    "bounds = np.linspace(0, max_label_id, len(labels))\n",
    "\n",
    "def display(display_list):\n",
    "  fig, cols = plt.subplots(1, len(display_list), sharey=True)\n",
    "  fig.set_size_inches(15, 5)\n",
    "  title = ['Input Image', 'Ground-truth Labels', 'Stage 1 Prediction Labels']\n",
    "  im = None\n",
    "  for i in range(len(display_list)):\n",
    "    cols[i].set_title(title[i])\n",
    "    y = display_list[i]\n",
    "    \n",
    "    if 'Label' in title[i]:\n",
    "      w,h,_ = y.shape\n",
    "      y = y.reshape(w,h)\n",
    "      im = cols[i].imshow(y, cmap=LabelCmap, vmin=0, vmax=max_label_id)\n",
    "    else:\n",
    "      im = cols[i].imshow(tf.keras.preprocessing.image.array_to_img(y)) # this look better\n",
    "      #im = cols[i].imshow(y)\n",
    "    cols[i].axis('off')\n",
    "  cbar = plt.colorbar(im, ax=cols.ravel().tolist(), cmap=LabelCmap, norm=colors.BoundaryNorm(bounds, LabelCmap.N), \n",
    "                      orientation='horizontal',spacing='proportional', ticks=bounds, boundaries=bounds, format='%1i')\n",
    "  cbar.ax.set_xticklabels([l.name for l in labels], rotation=90)\n",
    "  plt.show()\n",
    "\n",
    "# Plotting Loss, IoU, and Correct in separate subplots\n",
    "def plot_metrics(mode, loss_history, iou_history, correct_history):\n",
    "    #Tensor.cpu()\n",
    "    #print(loss_history)\n",
    "    #print(iou_history)\n",
    "    #print(correct_history)\n",
    "    epochs = range(1, len(loss_history) + 1)\n",
    "\n",
    "    # Create subplots: 1 row, 3 columns\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Plot Loss\n",
    "    ax1.plot(epochs, np.array(loss_history), color='tab:blue', label='Loss')\n",
    "    ax1.set_title(f\"{mode} Loss Over Epochs\")\n",
    "    ax1.set_xlabel('Images')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot IoU\n",
    "    #print(iou_history)\n",
    "    ax2.plot(epochs, [iou.cpu().numpy() for iou in iou_history], color='tab:green', label='IoU')\n",
    "    ax2.set_title(f\"{mode} IoU Over Epochs\")\n",
    "    ax2.set_xlabel('Images')\n",
    "    ax2.set_ylabel('IoU')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "    ax2.legend()\n",
    "\n",
    "    # Plot Correct Predictions\n",
    "    ax3.plot(epochs, np.array(correct_history), color='tab:red',\n",
    "             label='Correct Predictions')\n",
    "    ax3.set_title(f\"{mode} Correct Predictions Over Epochs\")\n",
    "    ax3.set_xlabel('Images')\n",
    "    ax3.set_ylabel('Correct Predictions')\n",
    "    ax3.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax3.legend()\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plots\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78743400-6296-45a8-9da3-50cbf8486543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import scipy.io as scio\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "class CrackDataSet(torch.utils.data.Dataset):\n",
    "    # data[0] is image, data[1] is mask\n",
    "    def __init__(self, data, data_transforms):\n",
    "        self.data_transforms = data_transforms\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data[0]) # images size\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # OpenCV's default = BGR => RGB and then adding normalization here\n",
    "        image = cv2.imread(self.data[0][index], cv2.IMREAD_COLOR)\n",
    "        image = self.data_transforms(Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype('uint8'), 'RGB'))\n",
    "\n",
    "        # masks are black and white images, so pixels closer to 255 consider = crack = label them\n",
    "        mask = cv2.imread(self.data[1][index], cv2.IMREAD_GRAYSCALE)\n",
    "        mask_label = np.zeros(mask.shape)\n",
    "        mask_label[mask >= 128] = 1\n",
    "        mask_label = np.asarray(mask_label, np.compat.long)\n",
    "        mask_label = torch.LongTensor(mask_label)\n",
    "        return image, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0ce063-df35-4694-8b5b-846e62098863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import JaccardIndex\n",
    "import shutil\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "def load_data(path=''):\n",
    "    images = sorted(glob(os.path.join(path, \"images/*\")))\n",
    "    masks = sorted(glob(os.path.join(path, \"masks/*\")))\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
    "    return (train_x, train_y), (valid_x, valid_y)\n",
    "\n",
    "\n",
    "class Trainer_Wrapper():\n",
    "    def __init__(self, model, input_path, batch_size, max_epochs, num_class=5):\n",
    "        self.num_class = num_class\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = 1  # An epoch in machine learning is one complete pass through the entire training dataset during the training process\n",
    "        self.max_epochs = max_epochs\n",
    "        self.lr = 0.001  # Learning rate for the optimizer\n",
    "        data_train, data_eval = load_data(path=input_path)\n",
    "        self.image_datasets = {\n",
    "            'train':\n",
    "                CrackDataSet(data_train, data_transforms['train']),\n",
    "            'validation':\n",
    "                CrackDataSet(data_eval, data_transforms['validation'])\n",
    "        }\n",
    "        # create dataloader to read batch_size of items for training\n",
    "        # when training, images should be choose random, so shuffle = True\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.image_datasets['train'],\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True)\n",
    "        self.val_loader = torch.utils.data.DataLoader(self.image_datasets['validation'],\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shuffle=False)\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)  # gradient descent algorithm\n",
    "\n",
    "        # Intersection over Union (IoU): Measures the overlap between the predicted segmentation and the ground truth.\n",
    "        self.fiou = JaccardIndex(task='multiclass', num_classes=self.num_class).to(self.device)\n",
    "        self.loss_criterion = nn.CrossEntropyLoss()\n",
    "        self.train_loss_history = []\n",
    "        self.train_iou_history = []\n",
    "        self.train_correct_history = []\n",
    "        self.eval_loss_history = []\n",
    "        self.eval_iou_history = []\n",
    "        self.eval_correct_history = []\n",
    "\n",
    "    def begin_train(self):\n",
    "        start = self.epoch\n",
    "        for self.epoch in range(start, self.max_epochs + 1):\n",
    "            self.training()\n",
    "            self.evaluating()\n",
    "        plot_metrics(\"Training\", self.train_loss_history, self.train_iou_history, self.train_correct_history)\n",
    "        plot_metrics(\"Evaluating\", self.eval_loss_history, self.eval_iou_history, self.eval_correct_history)\n",
    "\n",
    "    def training(self):\n",
    "        self.model.train()\n",
    "        total_correct = 0\n",
    "        # Wraps the DataLoader with tqdm for a progress bar during training.\n",
    "        total_iou, total_loss, total_num_of_images, data_bar = 0.0, 0.0, 0.0, tqdm(self.train_loader)\n",
    "        torch.enable_grad()  # Ensures that gradients are computed.\n",
    "        num_of_batches = 0\n",
    "        for data, target in data_bar:\n",
    "            num_of_batches += 1\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "            # ensures that all CUDA operations on the GPU have completed\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            out = self.model(data)\n",
    "\n",
    "            # Converts the model's output logits to predicted class labels.\n",
    "            # for example, tensor([[0.1, 2.5, 0.3],  # Sample 1 (class 1 has the highest score)\n",
    "            #                     [1.2, 0.4, 3.0]]) # Sample 2 (class 2 has the highest score)\n",
    "            # then calling argmax will get tensor([1, 2])\n",
    "            prediction = torch.argmax(out, dim=1)\n",
    "\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            # loss between the predictions and ground truth.\n",
    "            loss = self.loss_criterion(out, target)\n",
    "\n",
    "            num_of_samples = data.size(0)\n",
    "            # quantifies the difference between the predicted output and the true values\n",
    "            total_loss += loss.item() * num_of_samples\n",
    "            # Calculates the Intersection over Union (IoU) for the predictions.\n",
    "            total_iou += self.fiou(out.data.to(self.device), target.data.to(self.device))\n",
    "            total_num_of_images += num_of_samples\n",
    "\n",
    "            # the model parameters approach a local minimum in the loss landscape.\n",
    "            # Reducing the learning rate helps the model make smaller, more precise updates, which can improve convergence to a minimum.\n",
    "            self.lr = self.lr * 0.9997\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                param_group['lr'] = self.lr\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            # For example, if target has the shape [32, 256, 256] (for a batch of 32 images with 256x256 pixel labels, as in a segmentation task):\n",
    "            # target.numel() would return 32 * 256 * 256 = 2,097,152, which is the total number of elements in the tensor.\n",
    "            total_correct += torch.sum(prediction == target).item() / target.numel() * num_of_samples\n",
    "            # mPA = mean Pixel Accuracy\n",
    "\n",
    "            avg_loss = total_loss / total_num_of_images\n",
    "            avg_iou = total_iou / num_of_batches\n",
    "            avg_correct = total_correct / total_num_of_images\n",
    "\n",
    "            self.train_loss_history.append(avg_loss)\n",
    "            self.train_iou_history.append(avg_iou)\n",
    "            self.train_correct_history.append(avg_correct)\n",
    "            data_bar.set_description('Train epoch: [{}/{}] Loss: {:.4f} mPA: {:.2f}% IOU: {:.4f} '\n",
    "                                     .format(self.epoch, self.max_epochs, avg_loss, avg_correct, avg_iou))\n",
    "\n",
    "    def evaluating(self):\n",
    "        self.model.eval()  # enable eval mode\n",
    "        torch.no_grad()\n",
    "        total_iou, total_loss, total_correct, total_num_of_images, data_bar = 0.0, 0.0, 0.0, 0, tqdm(self.val_loader)\n",
    "        num_of_batches = 0\n",
    "        for data, target in data_bar:\n",
    "            num_of_batches += 1\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "            out = self.model(data)\n",
    "\n",
    "            prediction = torch.argmax(out, dim=1)\n",
    "            if (self.device.type == 'cuda'):\n",
    "                torch.cuda.synchronize()\n",
    "            loss = self.loss_criterion(out, target)\n",
    "\n",
    "            num_of_samples = data.size(0)\n",
    "            total_loss += loss.item() * num_of_samples\n",
    "            total_iou += self.fiou(out.data.to(self.device), target.data.to(self.device))\n",
    "            total_num_of_images += num_of_samples\n",
    "            total_correct += torch.sum(prediction == target).item() / target.numel() * num_of_samples\n",
    "\n",
    "            avg_loss = total_loss / total_num_of_images\n",
    "            avg_iou = total_iou / num_of_batches\n",
    "            avg_correct = total_correct / total_num_of_images\n",
    "\n",
    "            self.eval_loss_history.append(avg_loss)\n",
    "            self.eval_iou_history.append(avg_iou)\n",
    "            self.eval_correct_history.append(avg_correct)\n",
    "            data_bar.set_description('Eval epoch:  [{}/{}] Loss: {:.4f} mPA: {:.2f}% IOU: {:.4f} '\n",
    "                                     .format(self.epoch, self.max_epochs, avg_loss, avg_correct, avg_iou))\n",
    "\n",
    "        # self.random_val_predict()\n",
    "\n",
    "    def random_val_predict(self, info=True, idx=0):\n",
    "        if (idx == 0):\n",
    "            idx = random.randint(0, len(self.image_datasets['validation']) - 1)\n",
    "\n",
    "        rgb, mask = self.image_datasets['validation'][idx]\n",
    "        # Suppose image is a tensor with shape [1, 3, 480, 640]:\n",
    "        # 1 is the batch dimension (batch size of 1).\n",
    "        # 3 is the number of color channels (RGB).\n",
    "        # 480 and 640 are the height and width of the image.\n",
    "        # squeeze(0): Removes the first dimension if its size is 1\n",
    "        image = rgb.unsqueeze(0).to(self.device)\n",
    "        mask = mask.unsqueeze(0).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():  # Disables gradient computation\n",
    "            label = mask.cpu().numpy()  # convert a PyTorch tensor to a NumPy array and ensure it is on the CPU\n",
    "            label = label.transpose(1, 2, 0)\n",
    "            label = label.reshape(H, W, 1)\n",
    "\n",
    "            img = image.squeeze(0)\n",
    "            img = img.cpu().numpy()\n",
    "            img = img.transpose(1, 2, 0)\n",
    "            if img.shape[2] > 3:\n",
    "                img = img[:, :, 0:3]\n",
    "\n",
    "            outputs = self.model(image)\n",
    "            pred = torch.argmax(outputs, 1)  # Gets the predicted class labels.\n",
    "            pred = pred.squeeze(0).cpu().data.numpy()\n",
    "\n",
    "            display([img, label, pred.reshape(H, W, 1)])\n",
    "        return img, pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82689d8-3b35-49ef-8031-3ec92bfdb5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RUN THIS BEFORE AND AFTER YOU TRAIN AND EVAL YOUR MODEL TO FREE UP MEM, OTHERWISE YOU'LL END UP OOM ERROR\"\"\"\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4521cd43-0481-4fef-b010-25a7da34a54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch: [1/1] Loss: 0.2501 mPA: 0.94% IOU: 0.5205 :   4%|▉                      | 86/2260 [01:01<25:50,  1.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#model = UNet(3, num_of_classes, True)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer_Wrapper(model, input_path, batch_size, max_epochs, num_of_classes)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 62\u001b[0m, in \u001b[0;36mTrainer_Wrapper.begin_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     60\u001b[0m start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 62\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating()\n\u001b[0;32m     64\u001b[0m plot_metrics(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss_history, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_iou_history, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_correct_history)\n",
      "Cell \u001b[1;32mIn[5], line 113\u001b[0m, in \u001b[0;36mTrainer_Wrapper.training\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# For example, if target has the shape [32, 256, 256] (for a batch of 32 images with 256x256 pixel labels, as in a segmentation task):\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# target.numel() would return 32 * 256 * 256 = 2,097,152, which is the total number of elements in the tensor.\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m target\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m*\u001b[39m num_of_samples\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# mPA = mean Pixel Accuracy\u001b[39;00m\n\u001b[0;32m    116\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m total_num_of_images\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_path = 'kraggle_images'\n",
    "batch_size = 4 # Number of samples per batch during training\n",
    "max_epochs = 1\n",
    "num_of_classes = 2\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([transforms.ToTensor(), normalize]),\n",
    "    'validation': transforms.Compose([transforms.ToTensor(), normalize]),\n",
    "}\n",
    "\n",
    "model = UNet(3, num_of_classes)\n",
    "#model = UNet(3, num_of_classes, True)\n",
    "trainer = Trainer_Wrapper(model, input_path, batch_size, max_epochs, num_of_classes)\n",
    "trainer.begin_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a68058-80fe-43bf-9f56-056bdc712ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
